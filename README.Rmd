---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# SemanticDistance

<img src="man/figures/header4readme.png" alt="semantic relations between cat, dog, leash" width="40%" />

<!-- badges: start -->
[![R-CMD-check](https://github.com/Reilly-ConceptsCognitionLab/SemanticDistance/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/Reilly-ConceptsCognitionLab/SemanticDistance/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

SemanticDistance cleans and formats your target text (embedded in a dataframe). The package then computes pairwise metrics of cosine semantic distance between different adjacent chunks (e.g., ngrams, words, turns). The program appends two different semantic distance metrics, experiential and embedding. Experiential semantic distance reflects cosine (normalized from 0) between two vectors spanning 15 meaningful semantic dimensions (e.g., color, sound, valence). Embedding-based semantic distances are derived by contrasting each word's corresponding semantic vector spanning 300 hyperparameters as trained on the GLOVE word embedding model. The SemamticDistance package contains lookup databases with semantic vectors spanning >70k English words. <br/>

SemanticDistance operates on a dataframe that nominally has one column of text that has been split into a one word-per-row format. The package can also produce distance values for words arrayed in two columns. Users have numerous 'chunking' options for rolling distance comparisons in either monologues (no speaker information) or dialogues (speakers identifed as in conversation transcripts). Chunk options include: <br/>
1) word-to-word <br/>
2) ngram-to-ngram <br/>
3) ngram-to-word (rolling) <br/>
4) turn-to-turn (split by talker ID) <br/>

## Installation
Install the development version of SemanticDistance from [GitHub](https://github.com/) with:

``` r
install.packages("devtools")
devtools::install_github("Reilly-ConceptsCognitionLab/ConversationAlign")
```
You will nominally need at least one column of text within a dataframe. Ideally your text should be pre-formatted so that it is split into one word per row. However, this isn't critical. If all of your data are squished into a single string, SemanticDistance will split and unlist it so that it is formatted to one word per row. We need to do this to compute word-to-word distance! SemanticDistance can also compute pairwise distance for data arrayed in columns.

# Load Package
```{r, message=FALSE}
library(SemanticDistance)
```

# Step 1: Clean and Prep
SemanticDistance works on monologues (no talker information), dialogues (two or more speakers), or word pairs arrayed in columns. These functions will retain any metadata but also append identifiers that the distance functions will need later. Options for cleaning include lemmatization and omission of stopwords.<br/>
1) Read your transcript into R. It doesn't matter how you label your variables. You will specify these names in the arguments to your function calls. <br/>
2) Pick a cleaning function that matches the structure of your language transcript. The following code blocks illustrate these differences. <br/>

## Prep a Monologue (no talker info)
### View a messy monologue
This could be a story or instructions - anything where you don't care about talker information. You should format your transcript so that there is a vector (column) of words. All other metadata will be retained. 
Here's a sample monologue with all sorts of junk in a column called 'word'. The 'clean_monologue' function should split and append a unique identifier to each word while retaining empty strings that could be meaningful. Defaults are to omit stopwords and lemmatize.
```{r}
#raw messy transcript of a monologue with missing obs and text that needs to be split
data(MonologueSample1)
head(MonologueSample1, n=6)
```

### clean_monologue
Here's what you need to specify in the function cal: df, wordcol, omit_stops=TRUE, lemmatize = TRUE
df=dataframe name,  wordcol = quoted column name where your target text lives, omit_stops - omit stopwords default is TRUE, lemnatize - lemmatizes strings default is TRUE.
```{r}
my_clean_dat <- clean_monologue(MonologueSample1, 'word', omit_stops=T, lemmatize=T)
head(my_clean_dat, n=10)
```

## Prep a Dialogue (e.g.,conversation transcripts)
### View a messy sample dialogue
This could be a conversation transcript or any language sample where you care about talker/interlocutor information (e.g., computing semantic distance across turns in a conversation). 
```{r}
data("DialogueSample1")
head(DialogueSample1, n=6)
```

### clean_dialogue
Here's what you need to specify in the function call...<br/>
df = dataframe,  wordcol = column name (quoted) containing the text you want cleaned, whotalks = column name (quoted) containing the talker ID (will convert to factor), omit_stops = option for omitting stopwords, default is TRUE, lemmatize = option for lemmatizing strings, default is TRUE <br/>
```{r}
dyad <- clean_dialogue(DialogueSample1, "word", "speaker", omit_stops=T, lemmatize=T)
head(dyad, n=6)
```

## Prep Data Arrayed in Two Columns
SemanticDistance also computes pairwise distance for data arrayed in columns. 
```{r}
#Add ColumnSample usethis
```

## clean_columns
```{r}
data("ColumnSample")
head(ColumnSample, n=6)
```
<br/>
<br/>

# Steo 2: Compute Semantic Distance
Users have several options for chunking pairwise semantic distance comparisons. SemanticDistance contains two embedded lookup databases, each with coverage of >60k English words. One metric 'CosDist_SD15' reflects pairwise cosine distance between semantic vectors spanning 15 dimensions (e.g., color, sound, etc). Another metric (CosDist_Glo) reflects cosine distance between the 300-dimension embedding vectors tagged to each word (or averaged across the words within an ngram or turn).

## Option 1: rolling ngram-to-word
Users specify an ngram window size. This window rolls successively over your language sample to compute a semantic distance value for each new word relative to the n-words (ngram size) before it. This model of compouting distance is illustrated in the figure. The larger your specified ngram size the more smoothed the semantic vector will be over your language sample. 

<img src="man/figures/RollingNgramIllustrate.png" alt="illustrates how rolling ngrams work on a vector of words by moving a window and contrasting each chunk to each new word" width="60%" />
